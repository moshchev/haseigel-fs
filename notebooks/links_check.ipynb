{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how this data is generated? it is not on  github\n",
    "df_html = pd.read_parquet(\"../data/HTML_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_id</th>\n",
       "      <th>domain_start_id</th>\n",
       "      <th>response_url</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23884</td>\n",
       "      <td>3292</td>\n",
       "      <td>https://bianchi-industrial.it/products/linear-...</td>\n",
       "      <td>&lt;!doctypehtml&gt;&lt;html class=no-js lang=en-GB&gt;&lt;me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65092</td>\n",
       "      <td>1265</td>\n",
       "      <td>https://vilagrancha.com/en/1292-complete-clutc...</td>\n",
       "      <td>&lt;!doctypehtml&gt;&lt;html lang=en&gt;&lt;meta charset=utf-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reference_id  domain_start_id  \\\n",
       "0         23884             3292   \n",
       "1         65092             1265   \n",
       "\n",
       "                                        response_url  \\\n",
       "0  https://bianchi-industrial.it/products/linear-...   \n",
       "1  https://vilagrancha.com/en/1292-complete-clutc...   \n",
       "\n",
       "                                       response_text  \n",
       "0  <!doctypehtml><html class=no-js lang=en-GB><me...  \n",
       "1  <!doctypehtml><html lang=en><meta charset=utf-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_html.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_id</th>\n",
       "      <th>domain_start_id</th>\n",
       "      <th>response_url</th>\n",
       "      <th>response_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23884</td>\n",
       "      <td>3292</td>\n",
       "      <td>https://bianchi-industrial.it/products/linear-...</td>\n",
       "      <td>&lt;!doctypehtml&gt;&lt;html class=no-js lang=en-GB&gt;&lt;me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65092</td>\n",
       "      <td>1265</td>\n",
       "      <td>https://vilagrancha.com/en/1292-complete-clutc...</td>\n",
       "      <td>&lt;!doctypehtml&gt;&lt;html lang=en&gt;&lt;meta charset=utf-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reference_id  domain_start_id  \\\n",
       "0         23884             3292   \n",
       "1         65092             1265   \n",
       "\n",
       "                                        response_url  \\\n",
       "0  https://bianchi-industrial.it/products/linear-...   \n",
       "1  https://vilagrancha.com/en/1292-complete-clutc...   \n",
       "\n",
       "                                       response_text  \n",
       "0  <!doctypehtml><html class=no-js lang=en-GB><me...  \n",
       "1  <!doctypehtml><html lang=en><meta charset=utf-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib3\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Suppress SSL warnings globally\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_attributes(html):\n",
    "    \"\"\"\n",
    "    Extracts attributes from all <img> tags in the provided HTML content.\n",
    "    Returns a list of dictionaries, each containing attributes of an <img> tag.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    img_tags = soup.find_all('img')\n",
    "    img_data = [img.attrs for img in img_tags]  # List comprehension for brevity\n",
    "    \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_combined_html(df, output_file=\"../data/combined.html\"):\n",
    "    # Combine all response text into one HTML file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for html_content in df[\"response_text\"]:\n",
    "            file.write(html_content)\n",
    "            file.write(\"\\n\")  # Separate each HTML content by a newline for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_with_local_path(dict_list, base_url, download_folder=\"../data/images\"):\n",
    "    \"\"\"\n",
    "    Downloads images from URLs in img attributes list and stores them locally.\n",
    "    Adds the local path of each downloaded image back to the dictionary.\n",
    "    \"\"\"\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    default_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    \n",
    "    for index, img_data in enumerate(dict_list):\n",
    "        img_url = img_data.get(\"src\")\n",
    "        \n",
    "        # Convert relative URLs to absolute URLs\n",
    "        if img_url and urlparse(img_url).scheme == \"\":\n",
    "            img_url = urljoin(base_url, img_url)\n",
    "            print(f\"Converted relative URL to absolute: {img_url}\")\n",
    "\n",
    "        # Replace backslashes with forward slashes\n",
    "        if img_url:\n",
    "            img_url = img_url.replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        # Check if URL is valid and not a data URI\n",
    "        if img_url and urlparse(img_url).scheme in [\"http\", \"https\"]:\n",
    "            # Generate filename consistent with the original function\n",
    "            img_name = f\"{index}_{os.path.basename(urlparse(img_url).path)}\"\n",
    "            img_path = os.path.join(download_folder, img_name)\n",
    "            \n",
    "            # Download the image using requests with SSL verification disabled\n",
    "            try:\n",
    "                # Attempt to download with custom User-Agent on the first try\n",
    "                response = requests.get(img_url, headers=default_headers, stream=True, verify=False)\n",
    "                response.raise_for_status()  # Raise an error for HTTP issues\n",
    "                with open(img_path, \"wb\") as img_file:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        img_file.write(chunk)\n",
    "                print(f\"Downloaded: {img_name}\")\n",
    "                \n",
    "                # Add the relative path to the dictionary\n",
    "                img_data[\"local_path\"] = os.path.relpath(img_path, start=download_folder)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Print the error and skip to the next item\n",
    "                print(f\"Error downloading {img_url}: {e}\")\n",
    "                \n",
    "        else:\n",
    "            # Skip data URIs or invalid URLs\n",
    "            if img_url and img_url.startswith(\"data:image\"):\n",
    "                print(f\"Skipping item {index} - Data URI found.\")\n",
    "            else:\n",
    "                print(f\"Skipping item {index} - No valid image URL found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each HTML and base URL from the DataFrame\n",
    "all_images_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    html_content = row[\"response_text\"]\n",
    "    base_url = row[\"response_url\"]\n",
    "    \n",
    "    # Extract image attributes for the current HTML page\n",
    "    img_data = extract_img_attributes(html_content)\n",
    "    \n",
    "    # Download images and resolve paths\n",
    "    download_images_with_local_path(img_data, base_url=base_url)\n",
    "    \n",
    "    # Add base URL and image data to the overall list\n",
    "    for img in img_data:\n",
    "        img[\"source_url\"] = base_url  # Keep track of the page URL where the image was found\n",
    "    all_images_data.extend(img_data)\n",
    "\n",
    "# Optionally, convert all image data to a DataFrame for analysis\n",
    "img_df = pd.DataFrame(all_images_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_image_folder(folder_path=\"../data/images\"):\n",
    "    \"\"\"\n",
    "    Deletes all files in the specified folder that are not .jpg files or contain 'logo' in the filename (case-insensitive).\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check if file is not a .jpg or contains \"logo\" (case insensitive)\n",
    "        if not filename.lower().endswith(\".jpg\") or \"logo\" in filename.lower():\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {filename}: {e}\")\n",
    "\n",
    "# Usage\n",
    "clean_image_folder(\"../data/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating it into the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Changing process_html function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html(html, base_url, model):\n",
    "    \"\"\"nahui ne nuzhna -> replace with the download and classify\n",
    "\n",
    "    Args:\n",
    "        html (str): html code\n",
    "        model (MobileViTClassifier): keep this model as it is\n",
    "    Returns:\n",
    "       (dict) \n",
    "        html_results = {\n",
    "            \"predictions\": [],\n",
    "            \"statistics\": defaultdict(int)\n",
    "            }\n",
    "    \"\"\"\n",
    "    html_results = {\n",
    "        \"predictions\": [],\n",
    "        \"statistics\": defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    img_data = extract_img_attributes(html, base_url) # TODO -> replace the download_images_with_local_path -> adjust the workflow\n",
    "    # instead of putting single images in the function as an input dump whole list there.\n",
    "    for img in img_data:\n",
    "        # First download the image and add local path\n",
    "        download_images_with_local_path([img], TEMP_IMAGE_DIR)\n",
    "        \n",
    "        # Then check if download was successful and local path was added\n",
    "        if \"local_path\" in img:\n",
    "            # Skip non jpg files and logo images since they can't be processed by the model, check jpeg!!!\n",
    "            if not img[\"local_path\"].lower().endswith(\".jpg\") or \"logo\" in img[\"local_path\"].lower():\n",
    "                continue\n",
    "\n",
    "            ### from here you need to put this in the donwload & classify function\n",
    "            prediction = model.predict(img[\"local_path\"])['prediction']\n",
    "            \n",
    "            # Store individual prediction\n",
    "            html_results[\"predictions\"].append({\n",
    "                \"image_path\": img[\"local_path\"],\n",
    "                \"predicted_class\": prediction\n",
    "            })\n",
    "            \n",
    "            # Update statistics counter\n",
    "            html_results[\"statistics\"][prediction] += 1\n",
    "    \n",
    "    return html_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Changing download_images_with_local_path function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_attributes(html, base_url):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract attributes of all <img> tags and processes the 'src' attribute.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content.\n",
    "        base_url (str): The base URL to resolve relative paths in 'src' attributes.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing attributes of each <img> tag, with the 'src' modified.\n",
    "    \"\"\"\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # Find all <img> tags\n",
    "    img_tags = soup.find_all('img')\n",
    "\n",
    "    # Initialize list to store each img tag's attributes as dictionaries\n",
    "    img_data = []\n",
    "\n",
    "    # Loop through each img tag and extract attributes\n",
    "    for img in img_tags:\n",
    "        img_attributes = img.attrs  # Get all attributes of the img tag as a dictionary\n",
    "        img_url = img_attributes.get(\"src\")  # Get the 'src' attribute\n",
    "        \n",
    "        # Convert relative URLs to absolute URLs\n",
    "        if img_url and urlparse(img_url).scheme == \"\":\n",
    "            img_url = urljoin(base_url, img_url)\n",
    "            print(f\"Converted relative URL to absolute: {img_url}\")\n",
    "\n",
    "        # Replace backslashes with forward slashes\n",
    "        if img_url:\n",
    "            img_url = img_url.replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # Update the 'src' attribute in the dictionary\n",
    "        img_attributes[\"src\"] = img_url\n",
    "        img_data.append(img_attributes)  # Append dictionary to the list\n",
    "\n",
    "    return img_data\n",
    "\n",
    "def save_combined_html(df, output_file=\"../data/combined.html\"):\n",
    "    # Combine all response text into one HTML file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for html_content in df[\"response_text\"]:\n",
    "            file.write(html_content)\n",
    "            file.write(\"\\n\")  # Separate each HTML content by a newline for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_with_local_path(dict_list, download_folder=\"../data/images\"):\n",
    "    \"\"\"\n",
    "    Downloads images from URLs in a list of dictionaries and adds local file paths.\n",
    "    \n",
    "    Args:\n",
    "        dict_list (list): List of dictionaries containing image attributes.\n",
    "            Each dictionary has keys like 'src', 'alt', 'title', etc.\n",
    "            Example:\n",
    "            [\n",
    "                {\n",
    "                    'title': 'some title',\n",
    "                    'alt': 'some alt text',\n",
    "                    'border': '0', \n",
    "                    'src': 'https://example.com/images/logo.png' - URL to the image\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        download_folder (str): Path where images will be downloaded to.\n",
    "            Defaults to \"../data/images\"\n",
    "            \n",
    "    Returns:\n",
    "        None: Modifies the input dictionaries in-place by adding 'local_path' key\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the download folder exists\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    default_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    \n",
    "    # Loop through each dictionary in the list\n",
    "    for index, img_data in enumerate(dict_list):\n",
    "        img_url = img_data.get(\"src\")\n",
    "        \n",
    "        # Check if URL is valid and not a data URI\n",
    "        if img_url and urlparse(img_url).scheme in [\"http\", \"https\"]:\n",
    "            # Generate filename consistent with the original function\n",
    "            img_name = f\"{index}_{os.path.basename(urlparse(img_url).path)}\"\n",
    "            img_path = os.path.join(download_folder, img_name)\n",
    "            \n",
    "            # Download the image using requests with SSL verification disabled\n",
    "            try:\n",
    "                # Attempt to download with custom User-Agent on the first try\n",
    "                response = requests.get(img_url, headers=default_headers, stream=True, verify=False)\n",
    "                response.raise_for_status()  # Raise an error for HTTP issues\n",
    "                with open(img_path, \"wb\") as img_file:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        img_file.write(chunk)\n",
    "                print(f\"Downloaded: {img_name}\")\n",
    "                \n",
    "                # Add the relative path to the dictionary\n",
    "                img_data[\"local_path\"] = os.path.relpath(img_path, start=download_folder)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Print the error and skip to the next item\n",
    "                print(f\"Error downloading {img_url}: {e}\")\n",
    "        else:\n",
    "            # Skip data URIs or invalid URLs\n",
    "            if img_url and img_url.startswith(\"data:image\"):\n",
    "                print(f\"Skipping item {index} - Data URI found.\")\n",
    "            else:\n",
    "                print(f\"Skipping item {index} - No valid image URL found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
